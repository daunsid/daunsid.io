@incollection{example_invited_talk,
  author = {J. Doe},
  note = {Example Location},
  publisher = {Example Talk Name},
  year = {2040},
  keywords = "invited",
}

@software{example_software,
  author = {J. Doe},
  title = {Example software},
  url = {https://example.com},
  date = {2040},
}

@inproceedings{example_proceeding,
    Author = {Oryina K. Akputu1, Divine Dumzo-Ajufo, Christian C. Okafor, Adeboye Daniel Abayomi, Terhemba M. Ape},
    Year = {2023},
    Title = {Towards Development of a Multilingual Mobile Chat Application for Enhanced Global Communication},
    Note = {teknika.v13i1.717},
    doi = {0.34148},
    file = {language.pdf},
    abstract = {The advent of mobile chat applications has revolutionized everyday communication. These applications facilitate the exchange of  user's  textual  and  multimedia  content  across  languages  and  cultures.  Most  chat  applications  are  known  to  only  support  a limited set of predominantly spoken languages, thereby, leaving a substantial portion of the user population  without adequate multilingual support. This paper aims to bridge the linguistic gap by presenting Kobapp, a multilingual chat application. The Kobapp, leverages some of the cutting-edge technologies, such as React-Native, Next.js, and the DeepL API, to offer real-time, accurate translations while at the same time offering user privacy. The development process of the Kobapp is outlined from the system architecture and design, emphasizing the integration of a client-side (Android) and server-side using Node.js, Express.js, and MongoDB. Notably, user feedback plays a crucial role in shaping an application's features and functionality. Therefore, the application’s performance was evaluated through a conducted user study. Results of the study indicate a strong positive linear relationship between overall user satisfaction and translation accuracy for different language  pairs. Moreover, the  absence of outliers  and  the  model's  significance  further  reinforces  the  application's  commitment  to  data  quality  and  accuracy. Future research will explore new dimensions in multilingual communication and applications to promote a truly global communit},
}

@incollection{example_conference,
    author = {J. Doe},
    title = {Example talk},
    note = {Example Conference},
    year = {2040},
}

@unpublished{feynman06,
    Author = {Adeboye Daniel},
    Title = {Human activity recognition in wearables using random forest feature selection to improve model performance},
    Note = {ssrn.4257072},
    doi = {10.2139},
    file = {human-activity-recognition.pdf},
    abstract = {The entire study investigates how the selection of important features for training classification algorithms could affect the overall performance which includes the prediction accuracy and prediction runtime. The algorithms considered are support vector machine, logistic regression, and k-nearest neighbor. The work shows that selecting important features can decrease the runtime of the model which makes it memory efficient when deployed as a mobile application. The accuracy of the memory-efficient models using Random Forest which decreased only by a very small percentage is a good trade-off when compared to without using it. The model which achieves an accuracy of 92.7% using SVC shows that the Random Forest algorithm is a good method to select important features for classification algorithms. The accuracy of the three models could be improved by further tuning the parameters of the classification algorithms.},
}

@article{feynman1939forces,
  title={Music Recommendation Using Emotion Detection from Facial Expression in Portable Devices},
  author={Shehu Adamu, Oreofe Olurin, Daniel Adeboye, Garba Aliyu, Auwal Nataala, Augustine N. Egere Hashidu Baba, Bashir Timizi Yusuf},
  journal={SN Computer Science},
  year={2023},
  file={emotion-detection.pdf},
  abstract={ This  research  presents  a  novel  approach  to  music recommendation  by  utilizing  artificial  intelligence  for  real-time emotion detection from facial expressions, integrated into a portable device. The study acknowledges the profound connection between music and human emotions, highlighting the significance of employing facial expression analysis to enhance music recommendation systems. Leveraging the Yolo-V3 architecture as an object detection layer, the model is trained on a comprehensive facial  expression  database  (RafDB  datasets)  to  accurately  detect three primary emotional classes: happiness, sadness, and neutrality.  The  resultant  model  achieves  a  commendable  loss  of 1.69 after a  rigorous training regimen  spanning 100 epochs. The research culminates in the development of a music recommendation API that processes facial expression images through  a  POST  request,  providing  emotion  class,  confidence scores, and bounding box details. This valuable output directs the selection  of  music  tailored  to  the  detected  emotion,  seamlessly interfacing with Spotify’s Playlist API to retrieve pertinent songs and metadata. The API's integration into a portable ESP32 device, complemented  by  RGB  LED  color  displays  corresponding  to detected emotions, demonstrates a practical implementation of the model.  The  training  of  the  emotion  detection  model  utilized  the PyTorch library, while the API was constructed using the Python FastAPI web  framework.  By  presenting  an  effective  marriage  of emotion detection and music recommendation, this research illuminates  a  promising  trajectory  for  the  fusion  of  artificial intelligence, emotional awareness, and music, envisioning a more personalized and emotionally engaging user experience. },
}

